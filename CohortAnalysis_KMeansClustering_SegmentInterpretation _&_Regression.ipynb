{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"xxx.xlsx\")\n",
    "df['Dato'] = df['Dato'].map(lambda x: x.rstrip(' UTC'))\n",
    "df['Dato'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dato'] = pd.to_datetime(df['Dato'], format=\"%d.%m.%Y %H:%M:%S\").dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dato'] = pd.to_datetime(df['Dato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes\n",
    "# df.head()\n",
    "df[\"Dato\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First:\n",
    "# Define a function that will parse the date\n",
    "def get_day(x): return dt.datetime(x.year, x.month, x.day) \n",
    "\n",
    "# Create InvoiceDay column\n",
    "df['InvoiceDay'] = df['Dato'].apply(get_day)\n",
    "\n",
    "# Group by CustomerID and select the InvoiceDay value\n",
    "grouping = df.groupby('Gjennomføre')['InvoiceDay'] \n",
    "\n",
    "# Assign a minimum InvoiceDay value to the dataset\n",
    "df['CohortDay'] = grouping.transform('min')\n",
    "\n",
    "# View the top 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_int(df, column):\n",
    "    year = df[column].dt.year\n",
    "    month = df[column].dt.month\n",
    "    day = df[column].dt.day\n",
    "    return year, month, day\n",
    "\n",
    "# Get the integers for date parts from the `InvoiceDay` column\n",
    "invoice_year, invoice_month, invoice_day = get_date_int(df, \"InvoiceDay\")\n",
    "\n",
    "# Get the integers for date parts from the `CohortDay` column\n",
    "cohort_year, cohort_month, cohort_day = get_date_int(df, \"CohortDay\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference in years\n",
    "years_diff = invoice_year - cohort_year\n",
    "\n",
    "# Calculate difference in months\n",
    "months_diff = invoice_month - cohort_month\n",
    "\n",
    "# Calculate difference in days\n",
    "days_diff = invoice_day - cohort_day\n",
    "\n",
    "# Extract the difference in days from all previous values\n",
    "df['CohortIndex'] = years_diff * 365 + months_diff * 30 + days_diff + 1\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a groupby object as grouping DataFrame with this command: \n",
    "grouping = df.groupby(['CohortDay', 'CohortIndex'])\n",
    "\n",
    "# Count the number of unique values per customer ID\n",
    "cohort_data = grouping['Gjennomføre'].apply(pd.Series.nunique).reset_index()\n",
    "\n",
    "# Create a pivot \n",
    "cohort_counts = cohort_data.pivot(index='CohortDay', columns='CohortIndex', values='Gjennomføre')\n",
    "cohort_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first column and store it to cohort_sizes\n",
    "cohort_sizes = cohort_counts.iloc[:,0]\n",
    "\n",
    "# Divide the cohort count by cohort sizes along the rows\n",
    "retention = cohort_counts.divide(cohort_sizes, axis=0)\n",
    "retention = retention *100\n",
    "retention.round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import seaborn package as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize an 8 by 6 inches plot figure\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Average Return Rate by Monthly Cohorts')\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(data=retention, annot=True, cmap='Blues')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recency, Frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a hypothetical snapshot_day data so we can do the exercise as if we're doing analysis recently.\n",
    "snapshot_date = max(df.Dato) + dt.timedelta(days=1)\n",
    "\n",
    "# Aggregate data on a customer level\n",
    "datamart = df.groupby(['Gjennomføre']).agg({\n",
    "    'Dato': lambda x: (snapshot_date - x.max()).days,\n",
    "    'Gjennomføre': 'count'})\n",
    "\n",
    "# Rename columns for easier interpretation\n",
    "datamart.rename(columns = {'Dato': 'Recency',\n",
    "                           'Gjennomføre': 'Frequency'}, inplace=True)\n",
    "# Check the first rows\n",
    "datamart.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recency quartile\n",
    "# Create labels\n",
    "r_labels = range(4, 0, -1)\n",
    "# Assign these labels to four equal percentile groups\n",
    "r_quartiles = pd.qcut(datamart['Recency'], 4, labels = r_labels)\n",
    "# Create new columns R\n",
    "datamart = datamart.assign(R = r_quartiles.values)\n",
    "datamart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency and Monetary quartiles\n",
    "# Create labels\n",
    "f_labels = range(1,5)\n",
    "\n",
    "# Assign these labels to four equal percentile groups\n",
    "f_quartiles = pd.qcut(datamart['Frequency'], 4, labels = f_labels)\n",
    "\n",
    "# Create new columns F and M\n",
    "datamart = datamart.assign(F = f_quartiles.values)\n",
    "datamart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_rfm(x): return str(x['R']) + str(x['F']) \n",
    "datamart['RFM_Segment'] = datamart.apply(join_rfm, axis=1)\n",
    "datamart['RFM_Score'] = datamart[['R','F']].sum(axis=1)\n",
    "\n",
    "datamart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering on RFM segments\n",
    "# Largest RFM segments\n",
    "datamart.groupby('RFM_Segment').size().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and view the 15 best customers by RFM Score\n",
    "datamart.sort_values('RFM_Score', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and view the 15 worst customers by RFM Score\n",
    "datamart.sort_values('RFM_Score').head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart.groupby('RFM_Score').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': ['mean', 'count']}).round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping into named segments\n",
    "def segment_me(df):\n",
    "    if df['RFM_Score'] >= 8:\n",
    "        return '1. Gold'\n",
    "    elif (df['RFM_Score'] >= 5) and (df['RFM_Score'] < 8):\n",
    "        return '2. Silver'\n",
    "    else:\n",
    "        return '3. Bronze'\n",
    "# Create a new variable RFM_Level\n",
    "datamart['RFM_Level'] = datamart.apply(segment_me, axis=1)\n",
    "# Calculate average values for each RFM_Level, and return a size of each segment \n",
    "rfm_level_agg = datamart.groupby('RFM_Level').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': ['mean', 'count']\n",
    "}).round(1)\n",
    "\n",
    "# Print the aggregated dataset\n",
    "print(rfm_level_agg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing for K-means clustering\n",
    "Key k-means assumptions\n",
    "\n",
    "•\tSymmetric distribution of variables (not skewed)\n",
    "\n",
    "    o\tSkew removed with logarithmic transformation (Log = only on non-negative values)\n",
    "    \n",
    "•\tVariables with same average values\n",
    "\n",
    "•\tVariables with same variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart_rfm = datamart[['Recency', 'Frequency']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart_rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.distplot(datamart['Frequency'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "frequency_log = np.log(datamart['Frequency'])\n",
    "sns.distplot(frequency_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.distplot(datamart['Recency'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_log= np.log(datamart['Recency'])\n",
    "sns.distplot(frequency_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding the sequence\n",
    "#Unskew the data with log transformation\n",
    "import numpy as np\n",
    "datamart_log = np.log(datamart_rfm)\n",
    "\n",
    "# Normalize the variables with StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(datamart_log)\n",
    "\n",
    "# Scale and center the data\n",
    "#Store it separately for clustering\n",
    "datamart_normalized = scaler.transform(datamart_log)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "datamart_normalized = pd.DataFrame(datamart_normalized, index= datamart_log.index, columns= datamart_log.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart_normalized.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\tPractical implementation of k-means clustering\n",
    "Key steps\n",
    "\n",
    "•\tData pre-processing\n",
    "\n",
    "•\tChoosing a number of clusters\n",
    "\n",
    "•\tRunning k-means clustering on pre-processed data\n",
    "\n",
    "•\tAnalyzing average RFM values of each cluster\n",
    "\n",
    "Methods to define the number of clusters\n",
    "\n",
    "•\tVisual methods - elbow criterion\n",
    "\n",
    "•\tMathematical methods - silhouette coefficient\n",
    "\n",
    "•\tExperimentation and interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow criterion method\n",
    "# Plot the number of clusters against within-cluster \n",
    "#       sum-of-squared-errors (SSE) - sum of squared distances from every data point to their cluster center\n",
    "# Identify an \"elbow\" in the plot\n",
    "# Elbow - a point representing an \"optimal\" number of clusters\n",
    "# Import key libraries\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    " # Fit KMeans and calculate SSE for each *k*\n",
    "sse = {}\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "    kmeans.fit(datamart_normalized)\n",
    "    sse[k] = kmeans.inertia_ # sum of squared distances to closest cluster center\n",
    "\n",
    "\n",
    "\n",
    " # Plot SSE for each *k*\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('k'); plt.ylabel('SSE')\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()\n",
    "# Best to choose the point on elbow, or the next point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans from sklearn library and initialize it as kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=1)\n",
    "\n",
    "# Compute k-means clustering on pre-processed data\n",
    "kmeans.fit(datamart_normalized)\n",
    "\n",
    "# Extract cluster labels from labels_ attribute\n",
    "cluster_labels = kmeans.labels_\n",
    "#4.2. Analyzing average RFM values of each cluster\n",
    "# Create a cluster label column in the original DataFrame:\n",
    "datamart_rfm_k3 = datamart_rfm.assign(Cluster = cluster_labels)\n",
    "\n",
    "# Calculate average RFM values and segment size for each cluster:\n",
    "datamart_rfm_k3.groupby(['Cluster']).agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': ['mean', 'count']\n",
    "}).round(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.\tProfile and interpret segments\n",
    "\n",
    "Approaches to build customer personas\n",
    "\n",
    "•\tSummary statistics for each cluster e.g. average RFM values\n",
    "\n",
    "•\tSnake plots (from market research\n",
    "\n",
    "•\tRelative importance of cluster attributes compared to population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snake plots to understand and compare segments\n",
    "# Market research technique to compare different segments\n",
    "# Visual representation of each segment's attributes\n",
    "\n",
    "# Need to first normalize data (center & scale)\n",
    "# Plot each cluster's average normalized values of each attribute\n",
    "# Transform datamart_normalized as DataFrame and add a Cluster column\n",
    "datamart_normalized = pd.DataFrame(datamart_normalized, \n",
    "                                   index=datamart_rfm.index, \n",
    "                                   columns=datamart_rfm.columns)\n",
    "datamart_normalized['Cluster'] = datamart_rfm_k3['Cluster']\n",
    "\n",
    "# Melt the data into a long format so RFM values and metric names are stored in 1 column each\n",
    "datamart_melt = pd.melt(datamart_normalized.reset_index(), \n",
    "                    id_vars=['CustomerID', 'Cluster'],\n",
    "                    value_vars=['Recency', 'Frequency', 'MonetaryValue'], \n",
    "                    var_name='Attribute', \n",
    "                    value_name='Value')\n",
    "\n",
    "# Visualize the snake plot\n",
    "plt.title('Snake plot of standardized variables')\n",
    "sns.lineplot(x=\"Attribute\", y=\"Value\", hue='Cluster', data=datamart_melt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative importance of segment attributes\n",
    "# Useful technique to identify relative importance of each segment's attribute\n",
    "# Calculate average values of each cluster\n",
    "# Calculate average values of population\n",
    "# Calculate importance score by dividing them and subtracting 1\n",
    "cluster_avg = datamart_rfm_k3.groupby(['Cluster']).mean()\n",
    "population_avg = datamart_rfm.mean()\n",
    "relative_imp = cluster_avg / population_avg - 1\n",
    "\n",
    "# Analyze and plot relative importance\n",
    "# The further a ratio is from 0, the more important that attribute is for a segment relative to the total population.\n",
    "relative_imp.round(2)\n",
    "\n",
    "# Plot a heatmap for easier interpretation: \n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('Relative importance of attributes')\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='RdYlGn')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression for explanatory purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm # import statsmodels \n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'] = df['Dato'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars=[\"weekday\",\"Klassefisering\"]\n",
    "for var in cat_vars:\n",
    "    cat_list='var'+'_'+var\n",
    "    cat_list = pd.get_dummies(df[var], prefix=var)\n",
    "    data1=df.join(cat_list)\n",
    "    df=data1\n",
    "cat_vars=[\"weekday\",\"Klassefisering\"]\n",
    "data_vars=df.columns.values.tolist()\n",
    "to_keep=[i for i in data_vars if i not in cat_vars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst = ['xxx','xxx','xxx'....]\n",
    "\n",
    "lst2 = ['xxx','xxx','xxx'....]\n",
    "\n",
    "df = pd.merge(datamart,df[lst],left_index=True, right_on = \"Gjennomføre\",  how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = df[lst2]\n",
    "\n",
    "y = df[\"Recency\"]\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saturday and Klassefisering_VIEW_RAW_DATA are standard variables\n",
    "\n",
    "reg = smf.ols(formula = \"Recency ~ 'xxx','xxx','xxx'...\", data = df).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saturday and Klassefisering_VIEW_RAW_DATA are standard variables\n",
    "\n",
    "reg = smf.ols(formula = \"Frequency ~ 'xxx','xxx','xxx'...\", data = df).fit()\n",
    "reg.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
